{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from bert.load_midi_bert import load_model\n",
    "from finetune.finetune_dataset import FinetuneDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from finetune.finetune_trainer import FinetuneTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = '../bert/CP.pkl'\n",
    "ckpt_path = '../bert/pretrain_model.ckpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    data_root = '../data/'\n",
    "\n",
    "    if dataset not in ['pop909', 'pianist8', 'emopia']:\n",
    "        print(f'Dataset {dataset} not supported')\n",
    "        exit(1)\n",
    "\n",
    "    X_train = np.load(os.path.join(data_root, f'{dataset}/{dataset}_train.npy'), allow_pickle=True)\n",
    "    X_val = np.load(os.path.join(data_root, f'{dataset}/{dataset}_valid.npy'), allow_pickle=True)\n",
    "    X_test = np.load(os.path.join(data_root, f'{dataset}/{dataset}_test.npy'), allow_pickle=True)\n",
    "\n",
    "    print('X_train: {}, X_valid: {}, X_test: {}'.format(X_train.shape, X_val.shape, X_test.shape))\n",
    "\n",
    "    if dataset == 'pop909':\n",
    "        y_train = np.load(os.path.join(data_root, f'{dataset}/{dataset}_train_ans.npy'), allow_pickle=True)\n",
    "        y_val = np.load(os.path.join(data_root, f'{dataset}/{dataset}_valid_ans.npy'), allow_pickle=True)\n",
    "        y_test = np.load(os.path.join(data_root, f'{dataset}/{dataset}_test_ans.npy'), allow_pickle=True)\n",
    "    else:\n",
    "        y_train = np.load(os.path.join(data_root, f'{dataset}/{dataset}_train_ans.npy'), allow_pickle=True)\n",
    "        y_val = np.load(os.path.join(data_root, f'{dataset}/{dataset}_valid_ans.npy'), allow_pickle=True)\n",
    "        y_test = np.load(os.path.join(data_root, f'{dataset}/{dataset}_test_ans.npy'), allow_pickle=True)\n",
    "\n",
    "    print('y_train: {}, y_valid: {}, y_test: {}'.format(y_train.shape, y_val.shape, y_test.shape))\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "num_workers = 5\n",
    "index_layer = -1\n",
    "lr = 2e-5\n",
    "class_num = 3\n",
    "hs = 768\n",
    "cpu = False\n",
    "cuda_devices = [0,1,2,3]\n",
    "seq_class = True # composer, emotion, False = velocity, melody\n",
    "task = 'composer'\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dictionary\n",
      "X_train: (1186, 512, 4), X_valid: (156, 512, 4), X_test: (126, 512, 4)\n",
      "y_train: (1186,), y_valid: (156,), y_test: (126,)\n",
      "len of train_loader 99\n",
      "len of valid_loader 13\n",
      "len of valid_loader 11\n",
      "\n",
      "Creating Finetune Trainer using index layer -1\n",
      "   device: cuda\n",
      "init a fine-tune model, sequence-level task? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.10.6/envs/proj_env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "midibert = load_model(dict_path, ckpt_path)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_data('pianist8')\n",
    "trainset = FinetuneDataset(X=X_train, y=y_train)\n",
    "validset = FinetuneDataset(X=X_val, y=y_val)\n",
    "testset = FinetuneDataset(X=X_test, y=y_test)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "print(\"len of train_loader\",len(train_loader))\n",
    "valid_loader = DataLoader(validset, batch_size=batch_size, num_workers=num_workers)\n",
    "print(\"len of valid_loader\",len(valid_loader))\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, num_workers=num_workers)\n",
    "print(\"len of valid_loader\",len(test_loader))\n",
    "\n",
    "print(\"\\nCreating Finetune Trainer using index layer\", index_layer)\n",
    "trainer = FinetuneTrainer(midibert, train_loader, valid_loader, test_loader, index_layer, lr, class_num,\n",
    "                        hs, y_test.shape, cpu, cuda_devices, None, seq_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Start\n",
      "   save model at result/finetune/composer/composer model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]../aten/src/ATen/native/cuda/Loss.cu:186: nll_loss_forward_no_reduce_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:186: nll_loss_forward_no_reduce_cuda_kernel: block: [0,0,0], thread: [2,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:186: nll_loss_forward_no_reduce_cuda_kernel: block: [0,0,0], thread: [3,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:186: nll_loss_forward_no_reduce_cuda_kernel: block: [0,0,0], thread: [4,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:186: nll_loss_forward_no_reduce_cuda_kernel: block: [0,0,0], thread: [5,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:186: nll_loss_forward_no_reduce_cuda_kernel: block: [0,0,0], thread: [6,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:186: nll_loss_forward_no_reduce_cuda_kernel: block: [0,0,0], thread: [9,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:186: nll_loss_forward_no_reduce_cuda_kernel: block: [0,0,0], thread: [10,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:186: nll_loss_forward_no_reduce_cuda_kernel: block: [0,0,0], thread: [11,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.\n",
      "  0%|          | 0/99 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/root/code/HugoA45/music_project/music_project/notebooks/hugo_finetune.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/code/HugoA45/music_project/music_project/notebooks/hugo_finetune.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_dir, \u001b[39m'\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m outfile:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/code/HugoA45/music_project/music_project/notebooks/hugo_finetune.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m#outfile.write(\"Loading pre-trained model from \" + best_mdl.split('/')[-1] + '\\n')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/code/HugoA45/music_project/music_project/notebooks/hugo_finetune.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/code/HugoA45/music_project/music_project/notebooks/hugo_finetune.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         train_loss, train_acc \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/code/HugoA45/music_project/music_project/notebooks/hugo_finetune.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         valid_loss, valid_acc \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mvalid()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/code/HugoA45/music_project/music_project/notebooks/hugo_finetune.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         test_loss, test_acc, _ \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtest()\n",
      "File \u001b[0;32m~/code/HugoA45/music_project/music_project/notebooks/../finetune/finetune_trainer.py:62\u001b[0m, in \u001b[0;36mFinetuneTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 62\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miteration(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_data, \u001b[39m0\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSeqClass)\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m train_loss, train_acc\n",
      "File \u001b[0;32m~/code/HugoA45/music_project/music_project/notebooks/../finetune/finetune_trainer.py:117\u001b[0m, in \u001b[0;36mFinetuneTrainer.iteration\u001b[0;34m(self, training_data, mode, seq)\u001b[0m\n\u001b[1;32m    115\u001b[0m     y_hat \u001b[39m=\u001b[39m y_hat\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m    116\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(y_hat, y, attn, seq)\n\u001b[0;32m--> 117\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    119\u001b[0m \u001b[39m# udpate only in train\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining Start\")\n",
    "save_dir = os.path.join('result/finetune/', task)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "filename = os.path.join(save_dir, f'{task} model.ckpt')\n",
    "print(\"   save model at {}\".format(filename))\n",
    "\n",
    "best_acc, best_epoch = 0, 0\n",
    "bad_cnt = 0\n",
    "\n",
    "#    train_accs, valid_accs = [], []\n",
    "with open(os.path.join(save_dir, 'log'), 'a') as outfile:\n",
    "    #outfile.write(\"Loading pre-trained model from \" + best_mdl.split('/')[-1] + '\\n')\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = trainer.train()\n",
    "        valid_loss, valid_acc = trainer.valid()\n",
    "        test_loss, test_acc, _ = trainer.test()\n",
    "\n",
    "        is_best = valid_acc >= best_acc\n",
    "        best_acc = max(valid_acc, best_acc)\n",
    "\n",
    "        if is_best:\n",
    "            bad_cnt, best_epoch = 0, epoch\n",
    "        else:\n",
    "            bad_cnt += 1\n",
    "\n",
    "        print('epoch: {}/{} | Train Loss: {} | Train acc: {} | Valid Loss: {} | Valid acc: {} | Test loss: {} | Test acc: {}'.format(\n",
    "            epoch+1, epochs, train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc))\n",
    "\n",
    "#            train_accs.append(train_acc)\n",
    "#            valid_accs.append(valid_acc)\n",
    "        trainer.save_checkpoint(epoch, train_acc, valid_acc,\n",
    "                                valid_loss, train_loss, is_best, filename)\n",
    "\n",
    "\n",
    "        outfile.write('Epoch {}: train_loss={}, valid_loss={}, test_loss={}, train_acc={}, valid_acc={}, test_acc={}\\n'.format(\n",
    "            epoch+1, train_loss, valid_loss, test_loss, train_acc, valid_acc, test_acc))\n",
    "\n",
    "        if bad_cnt > 3:\n",
    "            print('valid acc not improving for 3 epochs')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
