{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.10.6/envs/proj_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dictionary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MidiBert(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (distance_embedding): Embedding(1023, 64)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (word_emb): ModuleList(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(4, 256)\n",
       "    )\n",
       "    (1): Embeddings(\n",
       "      (lut): Embedding(18, 256)\n",
       "    )\n",
       "    (2): Embeddings(\n",
       "      (lut): Embedding(88, 256)\n",
       "    )\n",
       "    (3): Embeddings(\n",
       "      (lut): Embedding(66, 256)\n",
       "    )\n",
       "  )\n",
       "  (in_linear): Linear(in_features=1024, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from transformers import BertConfig\n",
    "from model.model import MidiBert\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"Loading Dictionary\")\n",
    "with open('/root/code/HugoA45/music_project/music_project/model/CP.pkl', 'rb') as f:\n",
    "    e2w, w2e = pickle.load(f)\n",
    "\n",
    "# Define the configuration for the BERT model\n",
    "configuration = BertConfig(\n",
    "    max_position_embeddings=512,\n",
    "    position_embedding_type='relative_key_query',\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "midibert = MidiBert(bertConfig=configuration, e2w=e2w, w2e=w2e)\n",
    "\n",
    "# Define the path to your checkpoint here\n",
    "ckpt_path = '/root/code/HugoA45/music_project/music_project/model/pretrain_model.ckpt'\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "# Remove the unexpected key from the state dictionary\n",
    "if \"bert.embeddings.position_ids\" in checkpoint['state_dict']:\n",
    "    embeddings_position_ids =  checkpoint['state_dict'][\"bert.embeddings.position_ids\"]\n",
    "    del checkpoint['state_dict'][\"bert.embeddings.position_ids\"]\n",
    "\n",
    "# Load the state dictionary from the checkpoint into the model\n",
    "midibert.load_state_dict(checkpoint['state_dict'])\n",
    "midibert.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/root/code/HugoA45/music_project/music_project/data/emopia/emopia_test.npy')\n",
    "data_ans = np.load('/root/code/HugoA45/music_project/music_project/data/emopia/emopia_test_ans.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 512, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:10,:,:].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 0, 0, 3, 1, 3, 3, 1, 2, 1, 3, 2, 2, 1, 0, 0, 1, 2, 2, 3, 2,\n",
       "       3, 3, 1, 3, 3, 1, 1, 0, 3, 2, 1, 2, 0, 2, 0, 3, 1, 0, 3, 3, 3, 0,\n",
       "       1, 2, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 2, 2, 0, 2, 3, 1, 2, 2,\n",
       "       2, 1, 1, 0, 3, 0, 0, 2, 0, 0, 2, 0, 2, 2, 3, 2, 0, 1, 0, 3, 1, 0,\n",
       "       0, 3, 2, 2, 3, 0, 1, 3, 0, 1, 3, 0, 2, 0, 1, 3, 1, 3, 0, 2, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(1023, 64)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midibert.bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = midibert.forward(torch.from_numpy(data[0:5,0:512,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.last_hidden_state.shape\n",
    "# (batch_size, sequence_length, hidden_size)\n",
    "#\n",
    "#‘batch_size’ is the number of inputs you have,\n",
    "#‘sequence_length’ is the length of your input sequences, and\n",
    "#‘hidden_size’ is the size of the hidden layers in the BERT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_names = ['M', 'C', 'E','H','W','J','S','Y']\n",
    "composer_names = np.arange(0,8,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "midibert.classifier = torch.nn.Linear(configuration.hidden_size, len(composer_names))\n",
    "\n",
    "\n",
    "logits = midibert.classifier(res.last_hidden_state)\n",
    "\n",
    "\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "avg_probs = torch.mean(probs, dim=1)\n",
    "_, predicted_class = torch.max(avg_probs, 1)\n",
    "predicted_composers = [composer_names[idx.item()] for idx in predicted_class]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 2, 2, 2, 2],\n",
       " array([0, 3, 0, 0, 3, 1, 3, 3, 1, 2, 1, 3, 2, 2, 1, 0, 0, 1, 2, 2, 3, 2,\n",
       "        3, 3, 1, 3, 3, 1, 1, 0, 3, 2, 1, 2, 0, 2, 0, 3, 1, 0, 3, 3, 3, 0,\n",
       "        1, 2, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 2, 2, 0, 2, 3, 1, 2, 2,\n",
       "        2, 1, 1, 0, 3, 0, 0, 2, 0, 0, 2, 0, 2, 2, 3, 2, 0, 1, 0, 3, 1, 0,\n",
       "        0, 3, 2, 2, 3, 0, 1, 3, 0, 1, 3, 0, 2, 0, 1, 3, 1, 3, 0, 2, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_composers, data_ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(1023, 64)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midibert.bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109482240 parameters.\n"
     ]
    }
   ],
   "source": [
    "model = midibert.bert.from_pretrained('bert-base-uncased')\n",
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f'The model has {num_parameters} parameters.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In a typical BERT model, the state_dict keys for the transformer layers usually follow this pattern:\n",
    "\n",
    "bert.encoder.layer.{i}.{sub_layer}.{param}\n",
    "where:\n",
    "\n",
    "{i} is the layer index (from 0 to 11 for a 12-layer BERT model),\n",
    "{sub_layer} is the sub-layer type, which can be attention.self, attention.output, intermediate, or output,\n",
    "{param} is the parameter type, which can be weight or bias for linear layers, and gamma or beta for layer normalization.\n",
    "To count the number of unique transformer layers in the state_dict, you can extract the layer index from each key and add it to a set (which automatically removes duplicates), then count the number of elements in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 12 unique transformer layers.\n"
     ]
    }
   ],
   "source": [
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "# Initialize an empty set to store the layer indices\n",
    "layer_indices = set()\n",
    "\n",
    "# Iterate over the keys in the state dictionary\n",
    "for key in state_dict.keys():\n",
    "    # Split the key into parts\n",
    "    parts = key.split('.')\n",
    "    # If this key corresponds to a transformer layer\n",
    "    if parts[0] == 'bert' and parts[1] == 'encoder' and parts[2] == 'layer':\n",
    "        # Extract the layer index and add it to the set\n",
    "        layer_indices.add(int(parts[3]))\n",
    "\n",
    "# Print the number of unique layers\n",
    "print(f\"The model has {len(layer_indices)} unique transformer layers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_emb\n",
      "in_linear\n",
      "bert\n"
     ]
    }
   ],
   "source": [
    "# Get the names of the layers\n",
    "layer_names = state_dict.keys()\n",
    "\n",
    "# Get the unique keys from the layers\n",
    "unique_keys = set()\n",
    "for name in layer_names:\n",
    "    # Split the name by '.' and get the first part\n",
    "    key = name.split('.')[0]\n",
    "    unique_keys.add(key)\n",
    "\n",
    "# Print the unique keys\n",
    "for key in unique_keys:\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the state_dict keys for the transformer layers usually follow this pattern:\n",
    "\n",
    "bert.encoder.layer.{i}.{sub_layer}.{param}\n",
    "where:\n",
    "\n",
    "{i} is the layer index (from 0 to 11 for a 12-layer BERT model),\n",
    "{sub_layer} is the sub-layer type, which can be attention.self, attention.output, intermediate, or output,\n",
    "{param} is the parameter type, which can be weight or bias for linear layers, and gamma or beta for layer normalization.\n",
    "To count the number of unique transformer layers in the state_dict, you can extract the layer index from each key and add it to a set (which automatically removes duplicates), then count the number of elements in the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Bar', 'Position', 'Pitch', 'Duration'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2w.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'Bar New', 1: 'Bar Continue', 2: 'Bar <PAD>', 3: 'Bar <MASK>'},\n",
       " {0: 'Position 1/16',\n",
       "  1: 'Position 2/16',\n",
       "  2: 'Position 3/16',\n",
       "  3: 'Position 4/16',\n",
       "  4: 'Position 5/16',\n",
       "  5: 'Position 6/16',\n",
       "  6: 'Position 7/16',\n",
       "  7: 'Position 8/16',\n",
       "  8: 'Position 9/16',\n",
       "  9: 'Position 10/16',\n",
       "  10: 'Position 11/16',\n",
       "  11: 'Position 12/16',\n",
       "  12: 'Position 13/16',\n",
       "  13: 'Position 14/16',\n",
       "  14: 'Position 15/16',\n",
       "  15: 'Position 16/16',\n",
       "  16: 'Position <PAD>',\n",
       "  17: 'Position <MASK>'},\n",
       " {0: 'Pitch 22',\n",
       "  1: 'Pitch 23',\n",
       "  2: 'Pitch 24',\n",
       "  3: 'Pitch 25',\n",
       "  4: 'Pitch 26',\n",
       "  5: 'Pitch 27',\n",
       "  6: 'Pitch 28',\n",
       "  7: 'Pitch 29',\n",
       "  8: 'Pitch 30',\n",
       "  9: 'Pitch 31',\n",
       "  10: 'Pitch 32',\n",
       "  11: 'Pitch 33',\n",
       "  12: 'Pitch 34',\n",
       "  13: 'Pitch 35',\n",
       "  14: 'Pitch 36',\n",
       "  15: 'Pitch 37',\n",
       "  16: 'Pitch 38',\n",
       "  17: 'Pitch 39',\n",
       "  18: 'Pitch 40',\n",
       "  19: 'Pitch 41',\n",
       "  20: 'Pitch 42',\n",
       "  21: 'Pitch 43',\n",
       "  22: 'Pitch 44',\n",
       "  23: 'Pitch 45',\n",
       "  24: 'Pitch 46',\n",
       "  25: 'Pitch 47',\n",
       "  26: 'Pitch 48',\n",
       "  27: 'Pitch 49',\n",
       "  28: 'Pitch 50',\n",
       "  29: 'Pitch 51',\n",
       "  30: 'Pitch 52',\n",
       "  31: 'Pitch 53',\n",
       "  32: 'Pitch 54',\n",
       "  33: 'Pitch 55',\n",
       "  34: 'Pitch 56',\n",
       "  35: 'Pitch 57',\n",
       "  36: 'Pitch 58',\n",
       "  37: 'Pitch 59',\n",
       "  38: 'Pitch 60',\n",
       "  39: 'Pitch 61',\n",
       "  40: 'Pitch 62',\n",
       "  41: 'Pitch 63',\n",
       "  42: 'Pitch 64',\n",
       "  43: 'Pitch 65',\n",
       "  44: 'Pitch 66',\n",
       "  45: 'Pitch 67',\n",
       "  46: 'Pitch 68',\n",
       "  47: 'Pitch 69',\n",
       "  48: 'Pitch 70',\n",
       "  49: 'Pitch 71',\n",
       "  50: 'Pitch 72',\n",
       "  51: 'Pitch 73',\n",
       "  52: 'Pitch 74',\n",
       "  53: 'Pitch 75',\n",
       "  54: 'Pitch 76',\n",
       "  55: 'Pitch 77',\n",
       "  56: 'Pitch 78',\n",
       "  57: 'Pitch 79',\n",
       "  58: 'Pitch 80',\n",
       "  59: 'Pitch 81',\n",
       "  60: 'Pitch 82',\n",
       "  61: 'Pitch 83',\n",
       "  62: 'Pitch 84',\n",
       "  63: 'Pitch 85',\n",
       "  64: 'Pitch 86',\n",
       "  65: 'Pitch 87',\n",
       "  66: 'Pitch 88',\n",
       "  67: 'Pitch 89',\n",
       "  68: 'Pitch 90',\n",
       "  69: 'Pitch 91',\n",
       "  70: 'Pitch 92',\n",
       "  71: 'Pitch 93',\n",
       "  72: 'Pitch 94',\n",
       "  73: 'Pitch 95',\n",
       "  74: 'Pitch 96',\n",
       "  75: 'Pitch 97',\n",
       "  76: 'Pitch 98',\n",
       "  77: 'Pitch 99',\n",
       "  78: 'Pitch 100',\n",
       "  79: 'Pitch 101',\n",
       "  80: 'Pitch 102',\n",
       "  81: 'Pitch 103',\n",
       "  82: 'Pitch 104',\n",
       "  83: 'Pitch 105',\n",
       "  84: 'Pitch 106',\n",
       "  85: 'Pitch 107',\n",
       "  86: 'Pitch <PAD>',\n",
       "  87: 'Pitch <MASK>'},\n",
       " {0: 'Duration 0',\n",
       "  1: 'Duration 1',\n",
       "  2: 'Duration 2',\n",
       "  3: 'Duration 3',\n",
       "  4: 'Duration 4',\n",
       "  5: 'Duration 5',\n",
       "  6: 'Duration 6',\n",
       "  7: 'Duration 7',\n",
       "  8: 'Duration 8',\n",
       "  9: 'Duration 9',\n",
       "  10: 'Duration 10',\n",
       "  11: 'Duration 11',\n",
       "  12: 'Duration 12',\n",
       "  13: 'Duration 13',\n",
       "  14: 'Duration 14',\n",
       "  15: 'Duration 15',\n",
       "  16: 'Duration 16',\n",
       "  17: 'Duration 17',\n",
       "  18: 'Duration 18',\n",
       "  19: 'Duration 19',\n",
       "  20: 'Duration 20',\n",
       "  21: 'Duration 21',\n",
       "  22: 'Duration 22',\n",
       "  23: 'Duration 23',\n",
       "  24: 'Duration 24',\n",
       "  25: 'Duration 25',\n",
       "  26: 'Duration 26',\n",
       "  27: 'Duration 27',\n",
       "  28: 'Duration 28',\n",
       "  29: 'Duration 29',\n",
       "  30: 'Duration 30',\n",
       "  31: 'Duration 31',\n",
       "  32: 'Duration 32',\n",
       "  33: 'Duration 33',\n",
       "  34: 'Duration 34',\n",
       "  35: 'Duration 35',\n",
       "  36: 'Duration 36',\n",
       "  37: 'Duration 37',\n",
       "  38: 'Duration 38',\n",
       "  39: 'Duration 39',\n",
       "  40: 'Duration 40',\n",
       "  41: 'Duration 41',\n",
       "  42: 'Duration 42',\n",
       "  43: 'Duration 43',\n",
       "  44: 'Duration 44',\n",
       "  45: 'Duration 45',\n",
       "  46: 'Duration 46',\n",
       "  47: 'Duration 47',\n",
       "  48: 'Duration 48',\n",
       "  49: 'Duration 49',\n",
       "  50: 'Duration 50',\n",
       "  51: 'Duration 51',\n",
       "  52: 'Duration 52',\n",
       "  53: 'Duration 53',\n",
       "  54: 'Duration 54',\n",
       "  55: 'Duration 55',\n",
       "  56: 'Duration 56',\n",
       "  57: 'Duration 57',\n",
       "  58: 'Duration 58',\n",
       "  59: 'Duration 59',\n",
       "  60: 'Duration 60',\n",
       "  61: 'Duration 61',\n",
       "  62: 'Duration 62',\n",
       "  63: 'Duration 63',\n",
       "  64: 'Duration <PAD>',\n",
       "  65: 'Duration <MASK>'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2e['Bar'], w2e['Position'], w2e['Pitch'], w2e['Duration']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing the Input Data\n",
    "Tokenization: Convert your input text into tokens. BERT uses a specific tokenizer that breaks text into tokens understood by the model.\n",
    "Adding Special Tokens: BERT requires special tokens at the beginning and end of each sequence (e.g., [CLS] at the start and [SEP] at the end).\n",
    "Padding and Truncation: Sequences should be of the same length. You may need to pad shorter sequences with a special token (e.g., [PAD]) or truncate longer ones.\n",
    "Creating Attention Masks: Attention masks tell the model which tokens should be paid attention to and which are just padding.\n",
    "2. Loading the Pretrained BERT Model\n",
    "Select the Appropriate Model: Depending on your task (e.g., text classification, question answering), choose a BERT model pretrained for that task.\n",
    "Load the Model: Use a deep learning framework like PyTorch or TensorFlow to load the model. These frameworks often have APIs or libraries (like Hugging Face's Transformers) that simplify this process.\n",
    "3. Making a Prediction\n",
    "Input Preparation: Convert your preprocessed input data into a format compatible with your deep learning framework, typically tensors.\n",
    "Model Inference: Pass your input through the model. The model will output logits, which are raw prediction values.\n",
    "Interpreting Output: For classification tasks, apply a softmax function to the logits to get probabilities. For other tasks, the interpretation will vary.\n",
    "4. Postprocessing the Output\n",
    "Extract Predictions: Based on the task, extract the required information from the output (e.g., the class with the highest probability for classification tasks).\n",
    "Handling Special Cases: For more complex tasks like question answering, you may need additional postprocessing to extract the answer from the output tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
